 %iffalse

\let\negmedspace\undefined
\let\negthickspace\undefined
\documentclass[journal,12pt,onecolumn]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{txfonts}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{gensymb}
\usepackage{comment}
\usepackage[breaklinks=true]{hyperref}
\usepackage{tkz-euclide} 
\usepackage{listings}
\usepackage{gvv}

%\def\inputGnumericTable{}                                 
\usepackage[latin1]{inputenc}     
\usepackage{xparse}
\usepackage{color}                                            
\usepackage{array}                                            
\usepackage{longtable}                                       
\usepackage{calc}                                             
\usepackage{multirow}
\usepackage{multicol}
\usepackage{hhline}                                           
\usepackage{ifthen}                                           
\usepackage{lscape}
\usepackage{tabularx}
\usepackage{array}
\usepackage{float}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
\newcommand{\define}{\stackrel{\triangle}{=}}
\theoremstyle{remark}
\newtheorem{rem}{Remark}
% Marks the beginning of the document
\begin{document}
\title{{MATRIX THEORY - EE1030}\\
Software Assignment}

\author{ee24btech11058 - P.Shiny Diavajna}
\maketitle
\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}

 \textbf{Aim :} To compute the eigenvalues of a given matrix using C code.\\
 
\textbf{Chosen Algorithm:}\\

\textbf{QR Algorithm with Gram Schmidt Orthogonalization }
    \begin{itemize}
    \item   The QR algorithm is an iterative method for calculating eigenvalues of a matrix. It repeatedly factors a given matrix $A$ into a product of an orthogonal matrix $Q$ and an upper triangular matrix $R$, then updates $A$ as $A=RQ$. Over iterations, $A$ converges to a quasi-diagonal matrix, where the diagonal elements approximate the eigenvalues.

    \item \textbf{Gram-Schmidt Orthogonalization} is used to compute the orthogonal matrix $Q$ during $QR$ factorization. It ensures numerical stability by orthogonalizing the columns of $A$ explicitly.\\
     \end{itemize}

\textbf{Time Complexity:}

\begin{itemize}
    \item  \textbf{QR Factorization with Gram-Schmidt:} For an $n\times n$matrix, the Gram-Schmidt process has a complexity of $O(n^3)$.
    \item  \textbf{Overall QR Algorithm :} Since the $QR$ factorization is performed iteratively ,the total complexity depends on the number of iterations $k$. For $k$ iterations, the complexity is  $O(kn^3).$
    For matrices that converge quickly (e.g., symmetric matrices), $k$ is often small.\\
\end{itemize}

\textbf{Other Aspects :}
\begin{enumerate}
    \item \textbf{Memory Usage:}The QR algorithm requires storing the matrices $Q$ and $R$, making memory usage approximately $O(n^2)$.\\

    \item \textbf{Convergence Rate :} The convergence of the QR algorithm depends on the type of matrix 
    \begin{itemize}
        \item The algorithm converges rapidly , often quadratically, without additional enhancements.

        \item Convergence can be slower and may require shifts or deflation techniques to accelerate. The rate also depends on how well-separated the eigenvalues requiring more iterations.\\
        
    \end{itemize}

    \item \textbf{Suitability:}
    \begin{itemize}
        \item \textbf{Symmetric Matrices:} Excellent due to faster convergence and numerical stability.

        \item \textbf{Sparse Matrices:} the algorithm may not preserve sparsity, leading to increased computational costs.

        \item \textbf{Large Matrices:} High computational cost per iteration makes it less practical for very large matrices. However,it can handle moderately large matrices effectively.
    \end{itemize}
\end{enumerate} 

\newpage
\textbf{Comparison with other algorithms:}
 \begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Time Complexity} & \textbf{Accuracy} & \textbf{Suitability} \\ 
\hline
QR Algorithm & $O(kn^3)$ & High for all matrices & Best for dense, symmetric matrices \\ 
Power Iteration & $O(n^2k)$ & Good for dominant eigenvalue & Best for largest eigenvalue only \\ 
Jacobi Method & $O(n^3 \log n)$ & High for symmetric matrices & Best for symmetric matrices \\ 
Lanczos Method & $O(kn^2)$ & Moderate & Best for sparse symmetric matrices \\ 
Divide and Conquer & $O(n^3)$ & High & Suitable for symmetric matrices \\ 
\hline 
\end{tabular}
\end{table} \\



\textbf{Conclusion:} \\
The $QR$ algorithm with Gram-Schmidt orthogonalization is a robust and accurate method for eigenvalue computation, particularly for symmetric and dense matrices. However, its $O(kn^3)$ complexity makes it less efficient for very large or sparse matrices, where specialized methods like Lanczos are more suitable. For practical applications, matrix properties and computational resources dictate the choice of algorithm.


\end{document}
